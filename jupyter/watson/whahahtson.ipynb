{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>lang_abv</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5130fd2cb5</td>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b72532a0b</td>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3931fbe82a</td>\n",
       "      <td>Des petites choses comme celles-là font une di...</td>\n",
       "      <td>J'essayais d'accomplir quelque chose.</td>\n",
       "      <td>fr</td>\n",
       "      <td>French</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5622f0c60b</td>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86aaa48b45</td>\n",
       "      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n",
       "      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n",
       "      <td>th</td>\n",
       "      <td>Thai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>2b78e2a914</td>\n",
       "      <td>The results of even the most well designed epi...</td>\n",
       "      <td>All studies have the same amount of uncertaint...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>7e9943d152</td>\n",
       "      <td>But there are two kinds of  the pleasure of do...</td>\n",
       "      <td>But there are two kinds of the pleasure of doi...</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>5085923e6c</td>\n",
       "      <td>The important thing is to realize that it's wa...</td>\n",
       "      <td>It cannot be moved, now or ever.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>fc8e2fd1fe</td>\n",
       "      <td>At the west end is a detailed model of the who...</td>\n",
       "      <td>The model temple complex is at the east end.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>44301dfb14</td>\n",
       "      <td>For himself he chose Atat??rk, or Father of th...</td>\n",
       "      <td>Ataturk was the father of the Turkish nation.</td>\n",
       "      <td>en</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12120 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                            premise  \\\n",
       "0      5130fd2cb5  and these comments were considered in formulat...   \n",
       "1      5b72532a0b  These are issues that we wrestle with in pract...   \n",
       "2      3931fbe82a  Des petites choses comme celles-là font une di...   \n",
       "3      5622f0c60b  you know they can't really defend themselves l...   \n",
       "4      86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
       "...           ...                                                ...   \n",
       "12115  2b78e2a914  The results of even the most well designed epi...   \n",
       "12116  7e9943d152  But there are two kinds of  the pleasure of do...   \n",
       "12117  5085923e6c  The important thing is to realize that it's wa...   \n",
       "12118  fc8e2fd1fe  At the west end is a detailed model of the who...   \n",
       "12119  44301dfb14  For himself he chose Atat??rk, or Father of th...   \n",
       "\n",
       "                                              hypothesis lang_abv language  \\\n",
       "0      The rules developed in the interim were put to...       en  English   \n",
       "1      Practice groups are not permitted to work on t...       en  English   \n",
       "2                  J'essayais d'accomplir quelque chose.       fr   French   \n",
       "3      They can't defend themselves because of their ...       en  English   \n",
       "4        เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai   \n",
       "...                                                  ...      ...      ...   \n",
       "12115  All studies have the same amount of uncertaint...       en  English   \n",
       "12116  But there are two kinds of the pleasure of doi...       en  English   \n",
       "12117                   It cannot be moved, now or ever.       en  English   \n",
       "12118       The model temple complex is at the east end.       en  English   \n",
       "12119      Ataturk was the father of the Turkish nation.       en  English   \n",
       "\n",
       "       label  \n",
       "0          0  \n",
       "1          2  \n",
       "2          0  \n",
       "3          0  \n",
       "4          1  \n",
       "...      ...  \n",
       "12115      2  \n",
       "12116      0  \n",
       "12117      2  \n",
       "12118      2  \n",
       "12119      0  \n",
       "\n",
       "[12120 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = pd.read_csv(\"./train.csv\")\n",
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Des petites choses comme celles-là font une di...</td>\n",
       "      <td>J'essayais d'accomplir quelque chose.</td>\n",
       "      <td>French</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n",
       "      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n",
       "      <td>Thai</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>The results of even the most well designed epi...</td>\n",
       "      <td>All studies have the same amount of uncertaint...</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>But there are two kinds of  the pleasure of do...</td>\n",
       "      <td>But there are two kinds of the pleasure of doi...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>The important thing is to realize that it's wa...</td>\n",
       "      <td>It cannot be moved, now or ever.</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>At the west end is a detailed model of the who...</td>\n",
       "      <td>The model temple complex is at the east end.</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>For himself he chose Atat??rk, or Father of th...</td>\n",
       "      <td>Ataturk was the father of the Turkish nation.</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0      and these comments were considered in formulat...   \n",
       "1      These are issues that we wrestle with in pract...   \n",
       "2      Des petites choses comme celles-là font une di...   \n",
       "3      you know they can't really defend themselves l...   \n",
       "4      ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n",
       "...                                                  ...   \n",
       "12115  The results of even the most well designed epi...   \n",
       "12116  But there are two kinds of  the pleasure of do...   \n",
       "12117  The important thing is to realize that it's wa...   \n",
       "12118  At the west end is a detailed model of the who...   \n",
       "12119  For himself he chose Atat??rk, or Father of th...   \n",
       "\n",
       "                                              hypothesis language  label  \n",
       "0      The rules developed in the interim were put to...  English      0  \n",
       "1      Practice groups are not permitted to work on t...  English      2  \n",
       "2                  J'essayais d'accomplir quelque chose.   French      0  \n",
       "3      They can't defend themselves because of their ...  English      0  \n",
       "4        เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร     Thai      1  \n",
       "...                                                  ...      ...    ...  \n",
       "12115  All studies have the same amount of uncertaint...  English      2  \n",
       "12116  But there are two kinds of the pleasure of doi...  English      0  \n",
       "12117                   It cannot be moved, now or ever.  English      2  \n",
       "12118       The model temple complex is at the east end.  English      2  \n",
       "12119      Ataturk was the father of the Turkish nation.  English      0  \n",
       "\n",
       "[12120 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndt = dt.drop(['id','lang_abv'],axis=1)\n",
    "ndt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>language</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and these comments were considered in formulat...</td>\n",
       "      <td>The rules developed in the interim were put to...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>These are issues that we wrestle with in pract...</td>\n",
       "      <td>Practice groups are not permitted to work on t...</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you know they can't really defend themselves l...</td>\n",
       "      <td>They can't defend themselves because of their ...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>From Cockpit Country to St. Ann's Bay</td>\n",
       "      <td>From St. Ann's Bay to Cockpit Country.</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Look, it's your skin, but you're going to be i...</td>\n",
       "      <td>The boss will fire you if he sees you slacking...</td>\n",
       "      <td>English</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12115</th>\n",
       "      <td>The results of even the most well designed epi...</td>\n",
       "      <td>All studies have the same amount of uncertaint...</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12116</th>\n",
       "      <td>But there are two kinds of  the pleasure of do...</td>\n",
       "      <td>But there are two kinds of the pleasure of doi...</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12117</th>\n",
       "      <td>The important thing is to realize that it's wa...</td>\n",
       "      <td>It cannot be moved, now or ever.</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12118</th>\n",
       "      <td>At the west end is a detailed model of the who...</td>\n",
       "      <td>The model temple complex is at the east end.</td>\n",
       "      <td>English</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12119</th>\n",
       "      <td>For himself he chose Atat??rk, or Father of th...</td>\n",
       "      <td>Ataturk was the father of the Turkish nation.</td>\n",
       "      <td>English</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6870 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0      and these comments were considered in formulat...   \n",
       "1      These are issues that we wrestle with in pract...   \n",
       "3      you know they can't really defend themselves l...   \n",
       "7                  From Cockpit Country to St. Ann's Bay   \n",
       "8      Look, it's your skin, but you're going to be i...   \n",
       "...                                                  ...   \n",
       "12115  The results of even the most well designed epi...   \n",
       "12116  But there are two kinds of  the pleasure of do...   \n",
       "12117  The important thing is to realize that it's wa...   \n",
       "12118  At the west end is a detailed model of the who...   \n",
       "12119  For himself he chose Atat??rk, or Father of th...   \n",
       "\n",
       "                                              hypothesis language  label  \n",
       "0      The rules developed in the interim were put to...  English      0  \n",
       "1      Practice groups are not permitted to work on t...  English      2  \n",
       "3      They can't defend themselves because of their ...  English      0  \n",
       "7                 From St. Ann's Bay to Cockpit Country.  English      2  \n",
       "8      The boss will fire you if he sees you slacking...  English      1  \n",
       "...                                                  ...      ...    ...  \n",
       "12115  All studies have the same amount of uncertaint...  English      2  \n",
       "12116  But there are two kinds of the pleasure of doi...  English      0  \n",
       "12117                   It cannot be moved, now or ever.  English      2  \n",
       "12118       The model temple complex is at the east end.  English      2  \n",
       "12119      Ataturk was the father of the Turkish nation.  English      0  \n",
       "\n",
       "[6870 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_sevens = (dt['language'] == \"English\").sum()\n",
    "df = ndt[ndt['language'] == 'English']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['premise','hypothesis']]\n",
    "y = df[['label']]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Приведение к нижнему регистру\n",
    "    text = text.lower()\n",
    "    # Удаление пунктуации\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(text)\n",
    "    # Удаление стоп-слов\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Объединение токенов обратно в текст\n",
    "    processed_text = ' '.join(tokens)\n",
    "    return processed_text\n",
    "\n",
    "# Применение предобработки ко всей колонке\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8611</th>\n",
       "      <td>There are many homes built into the hillsides;...</td>\n",
       "      <td>The remaining homes that have not been convert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9396</th>\n",
       "      <td>Each of them was as tough as a thick tree and ...</td>\n",
       "      <td>They were tough and loyal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3954</th>\n",
       "      <td>i understand i can imagine you all have much t...</td>\n",
       "      <td>yes, up there the insects must be really annoy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>However, the other young lady was most kind.</td>\n",
       "      <td>I was told to leave immediately by the other y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4433</th>\n",
       "      <td>well what is it</td>\n",
       "      <td>Don't tell me about it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8120</th>\n",
       "      <td>HE KNOWS ABOUT THE MINES.</td>\n",
       "      <td>He has no idea that the mines exist.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9609</th>\n",
       "      <td>If I had chosen to be an actor, I should have ...</td>\n",
       "      <td>I did not choose to become an actor, as at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11457</th>\n",
       "      <td>At the top of the hill is the imposing medieva...</td>\n",
       "      <td>The medieval castle of Kadifekale is located a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11580</th>\n",
       "      <td>She admits to Dorcas, 'I don't know what to do...</td>\n",
       "      <td>She did not admit anything while speaking to D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9382</th>\n",
       "      <td>The narthex, or entrance hall to the nave, is ...</td>\n",
       "      <td>There is a tympanum of Jesus above the entranc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1374 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "8611   There are many homes built into the hillsides;...   \n",
       "9396   Each of them was as tough as a thick tree and ...   \n",
       "3954   i understand i can imagine you all have much t...   \n",
       "2630       However, the other young lady was most kind.    \n",
       "4433                                     well what is it   \n",
       "...                                                  ...   \n",
       "8120                           HE KNOWS ABOUT THE MINES.   \n",
       "9609   If I had chosen to be an actor, I should have ...   \n",
       "11457  At the top of the hill is the imposing medieva...   \n",
       "11580  She admits to Dorcas, 'I don't know what to do...   \n",
       "9382   The narthex, or entrance hall to the nave, is ...   \n",
       "\n",
       "                                              hypothesis  \n",
       "8611   The remaining homes that have not been convert...  \n",
       "9396                          They were tough and loyal.  \n",
       "3954   yes, up there the insects must be really annoy...  \n",
       "2630   I was told to leave immediately by the other y...  \n",
       "4433                             Don't tell me about it.  \n",
       "...                                                  ...  \n",
       "8120               He has no idea that the mines exist.   \n",
       "9609   I did not choose to become an actor, as at the...  \n",
       "11457  The medieval castle of Kadifekale is located a...  \n",
       "11580  She did not admit anything while speaking to D...  \n",
       "9382   There is a tympanum of Jesus above the entranc...  \n",
       "\n",
       "[1374 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Вадим\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_label.py:114: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Вадим\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_label.py:132: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5496,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "label_encoder = LabelEncoder()\n",
    "X_train['fit'] = (X_train['premise'] + ' ' + X_train['hypothesis']).apply(preprocess_text)\n",
    "X_test['fit'] = (X_test['premise'] + ' ' + X_test['hypothesis']).apply(preprocess_text)\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['fit'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['fit'])\n",
    "y_train_encoded = label_encoder.fit_transform(y_train) # one-hot encoder\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "y_train_encoded.shape # должен быть (5496, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4288.6714)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_encoded, dtype=torch.long)\n",
    "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
    "X_train_tensor.sum()\n",
    "X_test_tensor.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 100\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=bs, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 18)\n",
    "        self.fc2 = nn.Linear(18, 30)\n",
    "        self.fc3 = nn.Linear(30, 25)\n",
    "        self.fc4 = nn.Linear(25, 12)\n",
    "        self.fc5 = nn.Linear(12, output_size)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in (self.fc1, self.fc2, self.fc3, self.fc4, self.fc5):\n",
    "            x = i(x)\n",
    "            x = nn.Softmax()(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassifier(input_size=X_train_tensor.shape[1],\n",
    "                       output_size=len(label_encoder.classes_))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00005)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Вадим\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1013, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1019, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0986, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0943, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1010, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0996, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1025, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0915, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0985, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1013, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1050, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1005, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0998, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0924, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0955, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1015, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0989, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0979, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1029, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0957, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0978, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0952, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1042, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1015, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1056, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0960, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1016, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1035, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0872, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0980, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1029, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0917, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0941, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0954, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0948, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0940, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1019, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1008, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0950, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1031, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1058, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0961, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0999, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1013, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1015, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1020, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0924, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0986, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0965, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0892, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1042, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1028, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0970, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1005, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0965, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1072, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0958, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1003, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1037, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0910, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1046, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1006, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0999, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1035, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1017, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1035, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0949, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0965, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0985, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1033, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1006, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0978, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0895, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1035, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1030, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0949, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1062, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1037, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1069, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0936, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1032, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0985, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0958, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1019, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0971, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0980, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1009, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1037, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0920, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0962, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1003, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0982, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1013, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0930, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0940, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1003, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0898, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1034, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0988, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0973, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0895, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1009, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1000, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0999, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0944, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1010, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0988, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0975, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0939, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0985, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0991, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1045, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1002, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1041, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1003, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1005, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0966, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0958, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0980, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1056, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0939, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0992, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1052, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0929, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0971, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0925, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0940, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0922, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0975, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1026, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0988, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1006, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0961, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1027, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0952, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1011, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0955, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1072, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0954, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1006, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0945, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0998, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1064, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1020, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0985, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0945, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0910, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0975, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0996, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1030, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0939, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1006, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1032, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0977, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1015, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1016, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0941, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1045, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0980, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1008, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0938, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1019, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0937, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1015, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0889, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0947, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0957, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1076, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0932, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1024, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0973, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0978, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0940, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1006, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0939, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0904, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0999, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0978, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0918, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1023, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0867, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1053, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1032, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0922, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1037, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1060, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0941, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1000, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0911, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0936, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1009, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0962, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0979, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0937, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0927, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0953, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1070, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1030, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0979, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1011, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1025, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0998, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1009, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1019, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0991, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0986, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0960, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1049, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0916, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0998, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1026, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1005, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0948, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0991, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0978, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0923, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1078, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1002, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0936, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1005, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0962, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1030, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1009, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1000, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0948, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0955, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0986, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0973, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0971, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0955, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0992, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1007, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0935, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0952, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0991, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1032, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1000, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0965, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0977, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0988, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0958, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1006, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1034, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1003, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1030, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1028, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0950, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0937, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0982, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0998, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1000, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0878, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1079, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0973, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0991, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0954, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0979, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1026, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0982, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0986, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0938, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0989, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1082, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0930, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0971, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0915, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0930, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0932, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1026, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1033, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0908, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1032, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0949, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0989, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1091, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0992, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0972, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1015, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0958, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1010, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0978, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1011, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1003, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1017, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0936, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0990, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0947, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0980, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1008, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0966, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0993, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0961, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1039, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0948, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1049, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1044, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1001, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0973, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0993, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1062, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0950, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0973, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1007, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0960, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0955, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0966, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1026, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1008, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1000, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0998, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0955, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0996, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0967, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0939, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1002, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1040, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1014, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1024, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1038, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0982, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0981, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0973, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1035, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1077, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0990, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0970, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0944, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1048, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0979, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0917, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1005, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1044, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0953, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0951, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0957, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0993, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0950, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1011, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0977, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0961, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0966, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0989, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0988, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0947, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1027, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1010, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0970, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1003, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1000, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0931, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0893, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0991, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1033, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1080, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1045, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0954, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0962, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0971, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0993, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1011, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1033, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0954, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0996, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1055, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0988, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0942, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0990, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1050, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0922, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0994, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0961, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1017, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0936, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0983, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0970, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0975, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0950, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0947, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1026, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0959, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0947, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1036, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0993, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0999, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0921, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0907, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0976, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1019, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0849, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0963, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0928, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0997, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1012, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0987, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0979, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0937, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0984, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0971, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0990, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0908, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0946, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1018, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.1041, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0956, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0991, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0991, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0910, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0939, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0968, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0962, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0974, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0992, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0978, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0969, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0995, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0940, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.0939, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Вадим\\Desktop\\python\\jupyter\\watson\\whahahtson.ipynb Cell 13\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/%D0%92%D0%B0%D0%B4%D0%B8%D0%BC/Desktop/python/jupyter/watson/whahahtson.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/%D0%92%D0%B0%D0%B4%D0%B8%D0%BC/Desktop/python/jupyter/watson/whahahtson.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/%D0%92%D0%B0%D0%B4%D0%B8%D0%BC/Desktop/python/jupyter/watson/whahahtson.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/%D0%92%D0%B0%D0%B4%D0%B8%D0%BC/Desktop/python/jupyter/watson/whahahtson.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(loss)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    152\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    155\u001b[0m         group,\n\u001b[0;32m    156\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m         state_steps)\n\u001b[1;32m--> 163\u001b[0m     adam(\n\u001b[0;32m    164\u001b[0m         params_with_grad,\n\u001b[0;32m    165\u001b[0m         grads,\n\u001b[0;32m    166\u001b[0m         exp_avgs,\n\u001b[0;32m    167\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    168\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    169\u001b[0m         state_steps,\n\u001b[0;32m    170\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    172\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    173\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    174\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    175\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    176\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    177\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    178\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    179\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    180\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    181\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    182\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    183\u001b[0m     )\n\u001b[0;32m    185\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 311\u001b[0m func(params,\n\u001b[0;32m    312\u001b[0m      grads,\n\u001b[0;32m    313\u001b[0m      exp_avgs,\n\u001b[0;32m    314\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    315\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    316\u001b[0m      state_steps,\n\u001b[0;32m    317\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    318\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    319\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    320\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    321\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    322\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    323\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    324\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    325\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    326\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    327\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    384\u001b[0m exp_avg\u001b[39m.\u001b[39mlerp_(grad, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 385\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39;49maddcmul_(grad, grad\u001b[39m.\u001b[39;49mconj(), value\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m capturable \u001b[39mor\u001b[39;00m differentiable:\n\u001b[0;32m    388\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, _ in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_predictions.extend(predicted.numpy())\n",
    "\n",
    "all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = label_encoder.inverse_transform(all_predictions)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3027656477438137\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
