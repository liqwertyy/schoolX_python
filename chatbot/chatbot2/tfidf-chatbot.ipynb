{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Types Of Chatbot\n",
    "\n",
    "There are broadly two variants of chatbots: Rule-Based and Self learning.\n",
    "\n",
    "In a Rule-based approach, a bot answers questions based on some rules on which it is trained on. The rules defined can be very simple to very complex. The bots can handle simple queries but fail to manage complex ones.\n",
    "\n",
    "The Self learning bots are the ones that use some Machine Learning-based approaches and are definitely more efficient than rule-based bots. These bots can be of further two types: Retrieval Based or Generative\n",
    "\n",
    "i) In retrieval-based models, a chatbot uses some heuristic to select a response from a library of predefined responses. The chatbot uses the message and context of conversation for selecting the best response from a predefined list of bot messages. The context can include a current position in the dialog tree, all previous messages in the conversation, previously saved variables (e.g. username). Heuristics for selecting a response can be engineered in many different ways, from rule-based if-else conditional logic to machine learning classifiers.\n",
    "\n",
    "ii) Generative bots can generate the answers and not always replies with one of the answers from a set of answers. This makes them more intelligent as they take word by word from the query and generates the answers.\n",
    "\n",
    "We are building a retrieval-based chatbot (Using TF-IDF approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Вадим\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Вадим\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "f = open('chatbot.txt', 'r', errors = 'ignore')\n",
    "raw = f.read()\n",
    "raw = raw.lower()\n",
    "nltk.download('punkt') #tokenizer for english\n",
    "nltk.download('wordnet') #corpora\n",
    "sent_tokens = nltk.sent_tokenize(raw) #convert to list of sentences\n",
    "word_tokens = nltk.word_tokenize(raw) #convert to list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a chatbot (also known as a smartbot, talkbot, chatterbot, bot, im bot, interactive agent, conversational interface or artificial conversational entity) is a computer program or an artificial intelligence which conducts a conversation via auditory or textual methods.',\n",
       " '[1] such programs are often designed to convincingly simulate how a human would behave as a conversational partner, thereby passing the turing test.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokens[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'chatbot']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "#Wordnet is a semantically-oriented dictionary of English included in NLTK\n",
    "\n",
    "def lemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "def lemNormalize(text):\n",
    "    return lemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greeting by Keyword Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\",)\n",
    "\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greeting(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating response to a query using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    robo_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=lemNormalize, stop_words = 'english')\n",
    "    \n",
    "    # Get the TF IDF weighted Document-Term Matrix\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens) \n",
    "    # print(\"Document Count\", len(sent_tokens), \"Term Count\", len(TfidfVec.get_feature_names()))\n",
    "    # print(TfidfVec.get_feature_names())\n",
    "    \n",
    "    # Get the cosine similarity between user query and all the sentences in corpora. This will be a vector of similarities\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    \n",
    "    # Get the sentence (document) which matches the most with the query\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    \n",
    "    # Get the td-idf value of the index which matched the most\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    if(req_tfidf == 0):\n",
    "        robo_response =  \"I am sorry! I don't understand you\"\n",
    "    else:\n",
    "        robo_response = sent_tokens[idx]\n",
    "        \n",
    "    sent_tokens.remove(user_response)\n",
    "    return robo_response\n",
    "\n",
    "# response(\"conference chatbot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye! \n",
      "ROBO: hello\n",
      "ROBO: Bye! take care..\n"
     ]
    }
   ],
   "source": [
    "flag = True\n",
    "\n",
    "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye! \")\n",
    "\n",
    "while( flag == True):\n",
    "    user_response = input()\n",
    "    user_response = user_response.lower()\n",
    "    \n",
    "    if(user_response != 'bye'):\n",
    "        if(user_response == 'thanks' or user_response == 'thank_you'):\n",
    "            flag = False\n",
    "            print(\"ROBO: You are welcome..\")\n",
    "        else:\n",
    "            if(greeting(user_response) != None):\n",
    "                print(\"ROBO: \"+greeting(user_response))\n",
    "            else:\n",
    "                print(\"ROBO: \", end=\"\")\n",
    "                print(response(user_response))\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"ROBO: Bye! take care..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
